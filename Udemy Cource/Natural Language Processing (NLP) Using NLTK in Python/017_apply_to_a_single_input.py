# -*- coding: utf-8 -*-
"""017-Apply to a Single Input.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBX7YzbwEQg1gFVi8KSmKPeHFVB7MDz7
"""

import keras
from keras.datasets import imdb
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Flatten, Embedding
from numpy import array

(X_train, y_train) , (X_test, y_test) = imdb.load_data(num_words= 5000)

word_to_id = keras.datasets.imdb.get_word_index()
word_to_id = {k: (v + 3) for k, v in word_to_id.items()}
word_to_id["<PAD>"] = 0
word_to_id["<START>"] = 1
word_to_id["<UNK>"] = 2

X_train = sequence.pad_sequences(X_train, maxlen= 300)
X_test = sequence.pad_sequences(X_test, maxlen= 300)

model = Sequential()

model.add(Embedding(5000, 32, input_length= 300))
model.add(Flatten())
model.add(Dense(1, activation= "sigmoid"))

model.summary()

model.compile(optimizer= "adam",
              loss= "binary_crossentropy",
              metrics= ["accuracy"])

model.fit(X_train, y_train,
          batch_size= 64,
          epochs= 3,
          validation_data= (X_test, y_test))

neg = "this movie is bad"
pos = "i had fun"
neg_2 = "this movie was terrible"
pos_2 = "i realy liked the movie"

for review in [neg, pos, neg_2, pos_2]:
  temp = []
  for word in review.split(" "):
    temp.append(word_to_id[word])
  temp_padded = sequence.pad_sequences([temp], maxlen= 300)
  print(review + "-- Sent --" + str(model.predict(array([temp_padded][0]))[0][0]))